# SS é¡¹ç›®æ·±åº¦å®¡è®¡è¡¥å……æŠ¥å‘Š

## ç›®å½•

1. [é—æ¼çš„æ”¹è¿›ç©ºé—´](#é—æ¼çš„æ”¹è¿›ç©ºé—´) - 8 é¡¹
2. [æ½œåœ¨çš„è®¾è®¡é™·é˜±](#æ½œåœ¨çš„è®¾è®¡é™·é˜±) - 6 é¡¹
3. [æ‰©å±•æ€§ä¸ä¼¸ç¼©æ€§](#æ‰©å±•æ€§ä¸ä¼¸ç¼©æ€§) - 5 é¡¹
4. [è¿ç»´ä¸å¯è§‚æµ‹æ€§](#è¿ç»´ä¸å¯è§‚æµ‹æ€§) - 4 é¡¹
5. [API ä¸å‘åå…¼å®¹æ€§](#api-ä¸å‘åå…¼å®¹æ€§) - 3 é¡¹

---

## é—æ¼çš„æ”¹è¿›ç©ºé—´

### 1. **ç±»å‹æ³¨è§£è¦†ç›–åº¦ä¸å®Œå…¨** ğŸ”´ (ä¼˜å…ˆçº§ï¼šä¸­)

**å½“å‰çŠ¶æ€**ï¼š84.6% å‡½æ•°æœ‰è¿”å›ç±»å‹æ³¨è§£ï¼ˆ208/246ï¼‰ï¼Œç¼ºå¤± 38 ä¸ª

**é—®é¢˜**ï¼š
- æŸäº›å·¥å…·å‡½æ•°ä¸å†…éƒ¨æ–¹æ³•æ— è¿”å›ç±»å‹
- IDE è‡ªåŠ¨è¡¥å…¨ä¸ç±»å‹æ£€æŸ¥æ— æ³• 100% å·¥ä½œ
- æ–°è´¡çŒ®è€…æ˜“çŠ¯åŒæ ·é”™è¯¯

**ç¤ºä¾‹**ï¼š
```python
# src/infra/llm_tracing.py:39-47
def _sha256_hex(value: str) -> str:  # âœ“ æœ‰
    ...

def _estimate_tokens(text: str) -> int:  # âœ“ æœ‰
    ...

# å‡è®¾å­˜åœ¨æ— æ³¨è§£çš„ï¼š
def _parse_param(value):  # âœ— ç¼ºè¿”å›ç±»å‹
    return int(value) if value else None
```

**æ”¹è¿›æ–¹æ¡ˆ**ï¼š
```bash
# æ·»åŠ  mypy æˆ– pyright åˆ° CI/CD
pip install mypy
mypy src/ --strict --no-implicit-optional
```

**æ”¹è¿›æ¸…å•**ï¼š
- [ ] æ·»åŠ  `mypy>=1.8.0` åˆ° `pyproject.toml` çš„ dev ä¾èµ–
- [ ] é…ç½® `tool.mypy` æ®µï¼ˆstrict modeï¼‰
- [ ] è¡¥å…¨ 38 ä¸ªç¼ºå¤±çš„è¿”å›ç±»å‹
- [ ] åœ¨ CI ä¸­æ·»åŠ  `mypy` check
- **é¢„è®¡å·¥ä½œé‡**ï¼š3-4 å°æ—¶

---

### 2. **ä¾èµ–ç‰ˆæœ¬èŒƒå›´è¿‡å®½æ¾** ğŸŸ¡ (ä¼˜å…ˆçº§ï¼šä½)

**å½“å‰çŠ¶æ€**ï¼š
```toml
dependencies = [
    "fastapi>=0.110.0",      # â‰¥ 0.110.0ï¼ˆä»»æ„æ–°ç‰ˆæœ¬ï¼‰
    "pydantic>=2.6.0",       # â‰¥ 2.6.0ï¼ˆä»»æ„æ–°ç‰ˆæœ¬ï¼‰
    "uvicorn>=0.27.0",       # â‰¥ 0.27.0ï¼ˆä»»æ„æ–°ç‰ˆæœ¬ï¼‰
]
```

**é—®é¢˜**ï¼š
- `fastapi>=0.110.0` æ¶µç›–äº†æœªæ¥ 1.0+ çš„ä¸»ç‰ˆæœ¬æ›´æ–°ï¼Œå¯èƒ½å¼•å…¥ breaking changes
- å¼€å‘ç¯å¢ƒä¸ç”Ÿäº§ç¯å¢ƒä¾èµ–ä¸ä¸€è‡´æ—¶ï¼Œéš¾ä»¥å¤ç°é—®é¢˜
- ç‰ˆæœ¬è·³è·ƒå¯èƒ½å¯¼è‡´éšç§˜çš„è¡Œä¸ºå˜åŒ–

**æ”¹è¿›æ–¹æ¡ˆ**ï¼š
```toml
# ä½¿ç”¨é”å®šçš„ä¸»ç‰ˆæœ¬ï¼Œå…è®¸è¡¥ä¸/æ¬¡ç‰ˆæœ¬æ›´æ–°
dependencies = [
    "fastapi>=0.110.0,<1.0.0",
    "pydantic>=2.6.0,<3.0.0",
    "uvicorn>=0.27.0,<1.0.0",
]
```

**è¡¥å……**ï¼šæ·»åŠ  `pyproject.toml` çš„ lock æ–‡ä»¶ç®¡ç†
```bash
pip install pip-tools  # æˆ– poetry/uv
pip-compile pyproject.toml  # ç”Ÿæˆ requirements.txt.lock
```

**é¢„è®¡å·¥ä½œé‡**ï¼š1-2 å°æ—¶

---

### 3. **ç¼ºå°‘ Python ç‰ˆæœ¬æ”¿ç­–ä¸å‘åå…¼å®¹æ€§å£°æ˜** ğŸŸ¡ (ä¼˜å…ˆçº§ï¼šä½)

**å½“å‰çŠ¶æ€**ï¼š
```toml
requires-python = ">=3.12"
```

**é—®é¢˜**ï¼š
- æœªæ˜ç¡®è¯´æ˜æœ€ä½ç‰ˆæœ¬æ˜¯å¦ä¸ºç¨³å®šç‰ˆï¼ˆ3.12 GA æ˜¯ 2023-10ï¼‰
- æœªè¯´æ˜æ˜¯å¦è€ƒè™‘æ”¯æŒ 3.11ã€3.10ï¼ˆLTS åœºæ™¯ï¼‰
- æ–° API é‡‡ç”¨äº† Python 3.10+ ç‰¹æ€§ï¼ˆå¦‚ `|` union è¯­æ³•ï¼‰ï¼Œä¸æ¸…æ¥šä¸ºä½•å›ºå®š 3.12

**æ”¹è¿›**ï¼š
```python
# pyproject.toml æ·»åŠ æ³¨é‡Š
requires-python = ">=3.12"  
# ç†ç”±ï¼šä½¿ç”¨äº† 3.10+ çš„ | union type syntax å’Œ PEP 695 type parameters (3.12+)
# LTS æ”¿ç­–ï¼šæ”¯æŒ 3.12 LTS ç‰ˆæœ¬åŠä»¥ä¸Šã€‚å½“ 3.13/3.14 å‘å¸ƒæ—¶ï¼Œé€ç‰ˆæµ‹è¯•å¹¶æ›´æ–°çº¦æŸ
```

**é¢„è®¡å·¥ä½œé‡**ï¼š0.5 å°æ—¶ï¼ˆæ–‡æ¡£ï¼‰

---

### 4. **ç¼ºä¹æ•°æ®è¿ç§»/ç‰ˆæœ¬å‡çº§ç­–ç•¥** âœ… å·²è§£å†³ï¼ˆåŸä¼˜å…ˆçº§ï¼šé«˜ï¼‰

**çŠ¶æ€**ï¼šâœ… å·²è§£å†³ â€”â€” å·²å®ç° Job schema çš„ V1 â†’ V2 â†’ V3 è‡ªåŠ¨è¿ç§»ï¼ˆè¯»å…¼å®¹ã€å†™å…¥å½“å‰ç‰ˆæœ¬ï¼‰ã€‚

**å½“å‰å®ç°ï¼ˆä»£ç ï¼‰**ï¼š
- `src/domain/models.py`ï¼š`JOB_SCHEMA_VERSION_V1/V2/V3`ã€`JOB_SCHEMA_VERSION_CURRENT`ã€`SUPPORTED_JOB_SCHEMA_VERSIONS`
- `src/infra/job_store_migrations.py`ï¼š`assert_supported_schema_version()` + `migrate_payload_to_current()`ï¼ˆåŒ…å« `_migrate_v1_to_v2()`ã€`_migrate_v2_to_v3()`ï¼‰
- `src/infra/job_store.py`ï¼š`load()` è¯»å–åè¿ç§»ï¼›è‹¥å‘ç”Ÿè¿ç§»åˆ™ç”¨ `atomic_write_json()` åŸå­å›å†™åˆ°åŒä¸€ `job.json`

**è¡Œä¸ºè¯´æ˜**ï¼š
- è¯»ï¼šå…è®¸åŠ è½½ `schema_version in {1, 2, 3}`ï¼›æ—§ç‰ˆæœ¬ä¼šè¿ç§»åˆ° `JOB_SCHEMA_VERSION_CURRENT`
- å†™ï¼š`create()`/`save()` è¦æ±‚ `job.schema_version == JOB_SCHEMA_VERSION_CURRENT`ï¼Œé¿å…å†™å‡ºæ—§ schema
- è¿½è¸ªï¼šè¿ç§»æ—¶è®°å½• `SS_JOB_JSON_SCHEMA_MIGRATED`ï¼ˆå« `from_version`/`to_version`ï¼‰

**è¡¥å……**ï¼šè¿ç§»åçš„ payload ä¼šå›å†™åˆ°ç£ç›˜ï¼Œç¡®ä¿åç»­è¯»å–ä¸å†é‡å¤è¿ç§»ã€‚

**é¢„è®¡å·¥ä½œé‡**ï¼šâœ… å·²å®Œæˆ

---

### 5. **ç¼ºä¹å¹¶å‘æ§åˆ¶ä¸ç«æ€æ¡ä»¶é˜²æŠ¤** âœ… å·²è§£å†³ï¼ˆåŸä¼˜å…ˆçº§ï¼šé«˜ï¼‰

**çŠ¶æ€**ï¼šâœ… å·²è§£å†³ â€”â€” å·²å®ç°â€œæ–‡ä»¶é” + ä¹è§‚é”ï¼ˆ`version`ï¼‰+ åŸå­å†™å…¥â€ä¸‰å±‚é˜²æŠ¤ã€‚

**å½“å‰å®ç°ï¼ˆä»£ç ï¼‰**ï¼š
- `src/utils/file_lock.py`ï¼š`exclusive_lock()`ï¼ˆUnix `fcntl.flock`ï¼›Windows `msvcrt.locking`ï¼‰
- `src/domain/models.py`ï¼š`Job.version`ï¼ˆ`ge=1`ï¼‰
- `src/infra/job_store.py`ï¼š`JobStore.save()` ä½¿ç”¨ `job.json.lock` ä¸²è¡ŒåŒ–è¯»-æ”¹-å†™ï¼Œå¹¶æ ¡éªŒ/é€’å¢ `version`
- `src/infra/exceptions.py`ï¼š`JobVersionConflictError`ï¼ˆHTTP 409ï¼‰

**è¡Œä¸ºè¯´æ˜**ï¼š
- `save()` åœ¨æŒæœ‰ `job.json.lock` æ—¶è¯»å–æœ€æ–° `job.json`ï¼Œå¹¶å…ˆè¿ç§»åˆ°å½“å‰ schema åå†åšç‰ˆæœ¬æ ¡éªŒ
- å½“ `job.version != disk_version` æ—¶æ‹’ç»è¦†ç›–ï¼ŒæŠ›å‡º `JobVersionConflictError`
- å†™å…¥é‡‡ç”¨ `atomic_write_json()`ï¼ˆtempfile + `os.replace`ï¼‰ï¼Œé¿å…éƒ¨åˆ†å†™å…¥/æ–‡ä»¶æŸå

**é¢„è®¡å·¥ä½œé‡**ï¼šâœ… å·²å®Œæˆ

---

### 6. **ç¼ºä¹ä¼˜é›…å…³é—­ä¸èµ„æºæ¸…ç†** ğŸŸ¡ (ä¼˜å…ˆçº§ï¼šä¸­)

**å½“å‰çŠ¶æ€**ï¼š
```python
# src/main.py
def main() -> None:
    import uvicorn
    
    config = app.state.config
    log_config = build_logging_config(log_level=config.log_level)
    uvicorn.run(...)
```

**é—®é¢˜**ï¼š
- å¦‚æœ worker æ­£åœ¨å¤„ç† claimï¼Œçªç„¶å…³é—­ä¼šå¯¼è‡´ï¼š
  - claim æœªè¢« ackï¼Œjob é™·å…¥ RUNNING çŠ¶æ€
  - æ•°æ®åº“è¿æ¥æœªå…³é—­
  - LLM call ä¸­é€”ä¸­æ­¢
- æ—  shutdown hookï¼Œæ— ä¼˜é›…å…³é—­æµç¨‹

**æ”¹è¿›æ–¹æ¡ˆ**ï¼š

```python
# src/main.py
import signal
from contextlib import asynccontextmanager

shutdown_event = asyncio.Event()

@asynccontextmanager
async def lifespan(app: FastAPI):
    # å¯åŠ¨
    logger.info("SS_SERVER_STARTUP")
    yield
    
    # å…³é—­
    logger.info("SS_SERVER_SHUTDOWN_INITIATED")
    shutdown_event.set()
    
    # ç­‰å¾…ç°æœ‰ claim å®Œæˆï¼ˆæœ€å¤š 30 ç§’ï¼‰
    try:
        await asyncio.wait_for(
            _wait_for_claims_completion(),
            timeout=30.0
        )
    except asyncio.TimeoutError:
        logger.warning("SS_SERVER_SHUTDOWN_TIMEOUT")
    
    logger.info("SS_SERVER_SHUTDOWN_COMPLETE")

def create_app() -> FastAPI:
    app = FastAPI(title="SS", version="0.0.0", lifespan=lifespan)
    ...
    return app
```

**worker.py ä¸­æ·»åŠ ä¿¡å·å¤„ç†**ï¼š
```python
# src/worker.py
async def run_worker(config: Config) -> None:
    shutdown = asyncio.Event()
    
    def handle_shutdown(*_):
        logger.info("SS_WORKER_SHUTDOWN_REQUESTED")
        shutdown.set()
    
    signal.signal(signal.SIGTERM, handle_shutdown)
    signal.signal(signal.SIGINT, handle_shutdown)
    
    while not shutdown.is_set():
        try:
            await asyncio.wait_for(process_next(), timeout=1.0)
        except asyncio.TimeoutError:
            continue
    
    logger.info("SS_WORKER_SHUTDOWN_COMPLETE")
```

**é¢„è®¡å·¥ä½œé‡**ï¼š4-6 å°æ—¶

---

### 7. **ç¼ºä¹åˆ†å¸ƒå¼éƒ¨ç½²çš„ä¸€è‡´æ€§ä¿è¯** ğŸ”´ (ä¼˜å…ˆçº§ï¼šé«˜ï¼Œé˜¶æ®µäºŒ)

**å½“å‰çŠ¶æ€**ï¼š
- å•æœºæ–‡ä»¶å­˜å‚¨ (`jobs/` å’Œ `queue/` ç›®å½•)
- å¤šä¸ª API/worker å®ä¾‹å¯èƒ½åŒæ—¶è®¿é—®åŒä¸€ job

**é—®é¢˜**ï¼š
- å¦‚æœåˆ†å¸ƒå¼éƒ¨ç½²åˆ°å¤šå°æœºå™¨ï¼ŒNFS å¹¶å‘è®¿é—®ä¼šå‡ºç°ï¼š
  - æ–‡ä»¶ç¼“å­˜ä¸ä¸€è‡´
  - åŸå­æ€§ä¿è¯å¤±æ•ˆ
  - ç«æ€æ¡ä»¶æ¶åŒ–

**æ”¹è¿›æ–¹å‘**ï¼š
```python
# æŠ½è±¡ JobStore ä¸º Protocol
class JobStoreBackend(Protocol):
    def load(self, job_id: str) -> Job: ...
    def save(self, job: Job) -> None: ...

# æä¾›å¤šç§å®ç°
class FileJobStore(JobStoreBackend):
    """å•æœºæ–‡ä»¶å­˜å‚¨ï¼ˆå½“å‰ï¼‰"""
    ...

class RedisJobStore(JobStoreBackend):
    """åˆ†å¸ƒå¼ Redisï¼ˆæ¨èç”Ÿäº§ï¼‰"""
    def load(self, job_id: str) -> Job:
        data = self.redis.get(f"job:{job_id}")
        return Job.model_validate_json(data)
    
    def save(self, job: Job) -> None:
        self.redis.set(f"job:{job_id}", job.model_dump_json())

class PostgresJobStore(JobStoreBackend):
    """å…³ç³»æ•°æ®åº“ï¼ˆæ›¿ä»£æ–¹æ¡ˆï¼‰"""
    ...

# åœ¨ deps.py ä¸­æ³¨å…¥
def get_job_store() -> JobStoreBackend:
    backend = os.getenv("SS_JOB_STORE_BACKEND", "file")
    if backend == "redis":
        return RedisJobStore(...)
    elif backend == "postgres":
        return PostgresJobStore(...)
    else:
        return FileJobStore(...)
```

**é¢„è®¡å·¥ä½œé‡**ï¼š16-24 å°æ—¶ï¼ˆå«æµ‹è¯•ï¼‰

---

### 8. **ç¼ºä¹ API ç‰ˆæœ¬ç®¡ç†ä¸å¼ƒç”¨æ”¿ç­–** ğŸŸ¡ (ä¼˜å…ˆçº§ï¼šä¸­)

**å½“å‰çŠ¶æ€**ï¼š
```python
# src/api/routes.py
api_router = APIRouter()
api_router.include_router(jobs.router)
api_router.include_router(draft.router)
```

**é—®é¢˜**ï¼š
- API æœªä½¿ç”¨ç‰ˆæœ¬å‰ç¼€ï¼ˆå¦‚ `/v1/jobs` vs `/v2/jobs`ï¼‰
- æ— æ³•å¹¶è¡Œæ”¯æŒå¤šä¸ª API ç‰ˆæœ¬
- å¦‚æœ endpoint ç­¾åæ”¹å˜ï¼Œç°æœ‰å®¢æˆ·ç«¯ä¼šç ´å
- æ— å¼ƒç”¨é€šçŸ¥æœºåˆ¶ï¼ˆX-Deprecated-At header ç­‰ï¼‰

**æ”¹è¿›æ–¹æ¡ˆ**ï¼š

```python
# src/api/routes.py
from fastapi import APIRouter

api_v1_router = APIRouter(prefix="/v1", tags=["v1"])
api_v2_router = APIRouter(prefix="/v2", tags=["v2"])

# v1 endpointï¼ˆåç»­å¯æ ‡è®°ä¸º deprecatedï¼‰
@api_v1_router.post("/jobs")
async def create_job_v1(...):
    ...

# v2 endpointï¼ˆæ–°å¢å­—æ®µï¼‰
@api_v2_router.post("/jobs")
async def create_job_v2(...):
    ...

app.include_router(api_v1_router)
app.include_router(api_v2_router)
```

**æ·»åŠ å¼ƒç”¨è­¦å‘Š**ï¼š
```python
@api_v1_router.post("/jobs")
async def create_job_v1(...):
    """Deprecated: Use /v2/jobs instead."""
    return JSONResponse(
        status_code=200,
        headers={"Deprecation": "true", "Sunset": "2026-01-01"},
        content={...}
    )
```

**é¢„è®¡å·¥ä½œé‡**ï¼š3-4 å°æ—¶

---

## æ½œåœ¨çš„è®¾è®¡é™·é˜±

### 1. **LLM è°ƒç”¨çš„è¶…æ—¶ä¸é‡è¯•ç­–ç•¥ä¸æ˜ç¡®** ğŸŸ¡

**å½“å‰ä»£ç **ï¼š
```python
# src/infra/llm_tracing.py:80-120
async def draft_preview(self, *, job: Job, prompt: str) -> Draft:
    ...
    start = utc_now()
    try:
        draft = await self._inner.draft_preview(job=job, prompt=prompt)
    except LLMProviderError as e:
        ...
        raise LLMCallFailedError(...)
```

**é—®é¢˜**ï¼š
- æ— æ˜¾å¼è¶…æ—¶ï¼ˆawait å¯èƒ½æ°¸ä¹…æŒ‚èµ·ï¼‰
- æ— é‡è¯•ç­–ç•¥ï¼ˆç½‘ç»œæŠ–åŠ¨å°±å¤±è´¥ï¼‰
- æ— é™çº§æ–¹æ¡ˆï¼ˆLLM ä¸å¯ç”¨æ—¶çš„ç­–ç•¥ï¼‰

**æ”¹è¿›**ï¼š
```python
async def draft_preview(self, *, job: Job, prompt: str) -> Draft:
    timeout_sec = self._timeout or 30
    retries = 3
    
    for attempt in range(retries):
        try:
            draft = await asyncio.wait_for(
                self._inner.draft_preview(job=job, prompt=prompt),
                timeout=timeout_sec
            )
            return draft
        except asyncio.TimeoutError:
            logger.warning(
                "SS_LLM_TIMEOUT",
                extra={
                    "job_id": job.job_id,
                    "attempt": attempt + 1,
                    "timeout_sec": timeout_sec
                }
            )
            if attempt == retries - 1:
                raise LLMCallFailedError(...)
        except LLMProviderError as e:
            if attempt == retries - 1:
                raise
            await asyncio.sleep(2 ** attempt)  # exponential backoff
```

---

### 2. **State Machine å…è®¸éé¢„æœŸçš„è‡ªç¯è½¬ç§»** ğŸŸ¡

**å½“å‰ä»£ç **ï¼š
```python
# src/domain/state_machine.py:41-51
def ensure_transition(
    self,
    *,
    job_id: str,
    from_status: JobStatus,
    to_status: JobStatus,
) -> bool:
    if from_status == to_status:
        return False  # â† è‡ªç¯ä¸è½¬ç§»
    ...
```

**é—®é¢˜**ï¼š
- å¦‚æœè°ƒç”¨æ–¹è¿ç»­è°ƒç”¨ `ensure_transition(CREATED, CREATED)`ï¼Œè¿”å› False è€Œéå¼‚å¸¸
- å®¹æ˜“æ©ç›–é€»è¾‘é”™è¯¯ï¼ˆæœ¬åº”è½¬ç§»ï¼Œä½†å› ä¸ºçŠ¶æ€æœªæ›´æ–°è€Œå¡ä½ï¼‰

**æ”¹è¿›**ï¼š
```python
def ensure_transition(
    self,
    *,
    job_id: str,
    from_status: JobStatus,
    to_status: JobStatus,
) -> bool:
    # ç§»é™¤è‡ªç¯å®¹å¿ï¼Œè®©è°ƒç”¨æ–¹æ˜¾å¼æ£€æŸ¥
    if from_status == to_status:
        raise JobIllegalTransitionError(
            job_id=job_id,
            from_status=from_status,
            to_status=to_status,
        )  # å¼ºåˆ¶è°ƒç”¨æ–¹æ³¨æ„
    
    if not self.can_transition(from_status=from_status, to_status=to_status):
        raise JobIllegalTransitionError(...)
    
    return True  # ç®€åŒ–ï¼šæ€»æ˜¯è¿”å› True
```

---

### 3. **Worker Claim TTL ä¸é‡å¤„ç†é£é™©** ğŸŸ¡

**å½“å‰ä»£ç **ï¼š
```python
# src/infra/file_worker_queue.py
def claim(self, *, worker_id: str) -> QueueClaim | None:
    ...
    lease_expires_at = now + timedelta(seconds=self._lease_ttl_seconds)
    ...
```

**é—®é¢˜**ï¼š
- å¦‚æœ job æ‰§è¡Œè€—æ—¶ > lease_ttlï¼Œclaim è¿‡æœŸ
- å¦ä¸€ä¸ª worker ä¼šé‡æ–°å¤„ç†åŒä¸€ jobï¼ˆdouble executionï¼‰
- è™½ç„¶çŠ¶æ€æœºæä¾›äº†ä¸€äº›ä¿æŠ¤ï¼ˆå·² RUNNING çš„çŠ¶æ€æ£€æŸ¥ï¼‰ï¼Œä½†ä¸æ˜¯å®Œæ•´çš„å¹‚ç­‰æ€§

**æ”¹è¿›**ï¼š
```python
# åŠ¨æ€è°ƒæ•´ lease TTL
def claim(self, *, worker_id: str, estimated_duration: int = None) -> QueueClaim | None:
    ttl = estimated_duration or self._lease_ttl_seconds
    max_ttl = 3600  # 1 å°æ—¶ä¸Šé™
    ttl = min(ttl, max_ttl)
    lease_expires_at = now + timedelta(seconds=ttl)
    ...

# æˆ–è€…ï¼Œworker åœ¨æ‰§è¡Œä¸­å¿ƒè·³ï¼ˆå»¶æœŸ leaseï¼‰
def extend_claim(self, *, claim: QueueClaim, additional_seconds: int) -> None:
    ...
```

---

### 4. **PlanStep ä¾èµ–é“¾æœªéªŒè¯** ğŸŸ¡

**å½“å‰ä»£ç **ï¼š
```python
# src/domain/models.py:149-158
@field_validator("steps")
@classmethod
def steps_must_have_unique_ids_and_valid_deps(cls, steps: list[PlanStep]) -> list[PlanStep]:
    ids = [step.step_id for step in steps]
    if len(ids) != len(set(ids)):
        raise ValueError("steps contain duplicate step_id")
    known = set(ids)
    for step in steps:
        for dep in step.depends_on:
            if dep not in known:
                raise ValueError(f"unknown dependency: {dep}")
    return steps
```

**é—®é¢˜**ï¼š
- æ£€æŸ¥äº†ä¾èµ–æ˜¯å¦å­˜åœ¨ï¼Œä½†**æœªæ£€æŸ¥å¾ªç¯ä¾èµ–**ï¼ˆA â†’ B â†’ Aï¼‰
- æœªæ£€æŸ¥æ‹“æ‰‘æ’åºçš„å¯è¡Œæ€§
- æ‰§è¡Œæ—¶å¯èƒ½é™·å…¥æ— ç©·ç­‰å¾…

**æ”¹è¿›**ï¼š
```python
def _validate_plan_dag(steps: list[PlanStep]) -> None:
    """Check for cycles and topological ordering."""
    ids = [step.step_id for step in steps]
    
    # Check for cycles using DFS
    graph = {sid: step.depends_on for sid, step in zip(ids, steps)}
    visited = set()
    rec_stack = set()
    
    def has_cycle(node):
        visited.add(node)
        rec_stack.add(node)
        for dep in graph.get(node, []):
            if dep not in visited:
                if has_cycle(dep):
                    return True
            elif dep in rec_stack:
                return True
        rec_stack.remove(node)
        return False
    
    for step_id in ids:
        if step_id not in visited:
            if has_cycle(step_id):
                raise ValueError(f"circular dependency detected in plan: {step_id}")
```

---

### 5. **Artifact ç´¢å¼•å¯èƒ½æ¼‚ç§»** ğŸŸ¡

**å½“å‰ä»£ç **ï¼š
```python
# src/domain/worker_service.py:289-296
def _index_artifacts(self, *, job: Job, result: RunResult) -> None:
    known = {(ref.kind, ref.rel_path) for ref in job.artifacts_index}
    for ref in result.artifacts:
        key = (ref.kind, ref.rel_path)
        if key in known:
            continue
        job.artifacts_index.append(ref)
        known.add(key)
```

**é—®é¢˜**ï¼š
- artifacts_index åªåœ¨ memory ä¸­æ›´æ–°ï¼Œç„¶å save
- å¦‚æœ save å¤±è´¥ï¼Œç´¢å¼•ä¸æ–‡ä»¶ç³»ç»Ÿä¸åŒæ­¥
- æ¢å¤æ—¶æ— æ³•è‡ªåŠ¨é‡å»ºç´¢å¼•

**æ”¹è¿›**ï¼š
```python
def list_artifacts(self, *, job_id: str) -> list[ArtifactRef]:
    """Rebuild index from filesystem if needed."""
    job = self._store.load(job_id)
    job_dir = self._get_job_dir(job_id)
    
    actual_files = set(job_dir.rglob("*"))
    indexed_files = {self._resolve_artifact_path(job_id, ref.rel_path) for ref in job.artifacts_index}
    
    if actual_files != indexed_files:
        logger.warning(
            "SS_ARTIFACTS_INDEX_DRIFT",
            extra={
                "job_id": job_id,
                "missing": [str(f.relative_to(job_dir)) for f in actual_files - indexed_files],
            }
        )
        # é‡å»ºç´¢å¼•
        for file_path in actual_files:
            rel_path = str(file_path.relative_to(job_dir))
            if not any(ref.rel_path == rel_path for ref in job.artifacts_index):
                job.artifacts_index.append(ArtifactRef(kind=ArtifactKind.UNKNOWN, rel_path=rel_path))
        self._store.save(job)
    
    return job.artifacts_index
```

---

### 6. **Config éªŒè¯ç¼ºä¹ç»†ç²’åº¦æ£€æŸ¥** ğŸŸ¡

**å½“å‰ä»£ç **ï¼š
```python
# src/config.py:25-36
def _int_value(raw: str, *, default: int) -> int:
    try:
        return int(raw)
    except (TypeError, ValueError):
        return default

def _float_value(raw: str, *, default: float) -> float:
    try:
        return float(raw)
    except (TypeError, ValueError):
        return default
```

**é—®é¢˜**ï¼š
- æ— éªŒè¯èŒƒå›´ï¼ˆå¦‚ queue_lease_ttl_seconds å¯ä»¥æ˜¯è´Ÿæ•°ï¼‰
- worker_max_attempts å¯ä»¥æ˜¯ 0ï¼ˆæ— æ³•æ‰§è¡Œï¼‰
- stata_cmd å¯ä»¥æ˜¯ä¸å­˜åœ¨çš„å‘½ä»¤ï¼ˆè¿è¡Œæ—¶æ‰å¤±è´¥ï¼‰

**æ”¹è¿›**ï¼š
```python
@dataclass(frozen=True)
class Config:
    jobs_dir: Path
    ...
    
    def __post_init__(self):
        # éªŒè¯ç›®å½•å­˜åœ¨æˆ–å¯åˆ›å»º
        try:
            self.jobs_dir.mkdir(parents=True, exist_ok=True)
        except OSError as e:
            raise ValueError(f"Cannot create jobs_dir: {e}") from e
        
        # éªŒè¯èŒƒå›´
        if self.queue_lease_ttl_seconds <= 0:
            raise ValueError("queue_lease_ttl_seconds must be positive")
        
        if self.worker_max_attempts <= 0:
            raise ValueError("worker_max_attempts must be at least 1")
        
        # éªŒè¯ stata_cmd
        if self.stata_cmd:
            cmd_path = shutil.which(self.stata_cmd[0])
            if not cmd_path:
                raise ValueError(f"stata command not found: {self.stata_cmd[0]}")
```

---

## æ‰©å±•æ€§ä¸ä¼¸ç¼©æ€§

### 1. **é˜Ÿåˆ—ååé‡è®¾è®¡ä¸æ¸…æ™°** ğŸŸ¡

**é—®é¢˜**ï¼š
- å½“å‰ FileWorkerQueue åŸºäºæ–‡ä»¶ç³»ç»Ÿï¼Œå•æœºæ€§èƒ½ä¸Šé™
- æ— æ³•åŠ¨æ€æ‰©å±• worker æ•°é‡ï¼ˆæ— è´Ÿè½½å‡è¡¡ï¼‰
- æ— ä¼˜å…ˆçº§é˜Ÿåˆ—æ”¯æŒ

**å»ºè®®**ï¼š
```python
# Phase 2: æ”¯æŒæ¶ˆæ¯é˜Ÿåˆ—
class WorkerQueue(Protocol):
    async def enqueue(self, *, job_id: str, priority: int = 0) -> None: ...
    async def claim(self, *, worker_id: str, timeout: float = 1.0) -> QueueClaim | None: ...

# å®ç°ï¼šRedis Stream / RabbitMQ / AWS SQS
class RedisStreamQueue(WorkerQueue):
    """æ”¯æŒä¼˜å…ˆçº§ã€TTLã€æ¶ˆè´¹ç»„"""
    ...

class RabbitMQQueue(WorkerQueue):
    """æ”¯æŒä¼˜å…ˆçº§ã€æ­»ä¿¡é˜Ÿåˆ—ã€ç¡®è®¤æœºåˆ¶"""
    ...
```

---

### 2. **Job Store åˆ†ç‰‡ç­–ç•¥ç¼ºå¤±** ğŸŸ¡

**é—®é¢˜**ï¼š
- å½“ job æ•°é‡è¾¾åˆ°ç™¾ä¸‡çº§ï¼Œå•ä¸ª jobs/ ç›®å½•æ— æ³•æ‰¿è½½
- æ–‡ä»¶ç³»ç»Ÿæœç´¢æ€§èƒ½ä¸‹é™

**æ”¹è¿›**ï¼š
```python
def _job_dir(self, job_id: str) -> Path:
    # å“ˆå¸Œåˆ†ç‰‡ï¼šjob_id é¦– 2 å­—ç¬¦ä½œä¸ºç›®å½•
    shard = job_id[:2]
    return self._jobs_dir / shard / job_id
```

---

### 3. **ç¼ºä¹èµ„æºéš”ç¦»ä¸é…é¢** ğŸŸ¡

**é—®é¢˜**ï¼š
- æŸä¸ªç”¨æˆ·çš„å¤§é‡ job å¯èƒ½è€—å°½ç³»ç»Ÿèµ„æº
- æ— é€Ÿç‡é™åˆ¶ã€æ—  quota

**æ”¹è¿›**ï¼š
```python
# åœ¨ API layer æ·»åŠ 
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@router.post("/jobs")
@limiter.limit("10/minute")  # é™åˆ¶é¢‘ç‡
async def create_job(...):
    ...
```

---

### 4. **ç¼ºä¹åŠ¨æ€é…ç½®ä¸çƒ­é‡è½½** ğŸŸ¡

**é—®é¢˜**ï¼š
- ä¿®æ”¹é…ç½®éœ€è¦é‡å¯æœåŠ¡
- æ— æ³•åŠ¨æ€è°ƒæ•´æ—¥å¿—çº§åˆ«ã€worker æ•°é‡ç­‰

**æ”¹è¿›**ï¼š
```python
# src/infra/config_manager.py
class ConfigManager:
    def get_config(self) -> Config: ...
    def set_log_level(self, level: str) -> None: ...
    def get_dynamic_config(self, key: str) -> Any: ...

# åœ¨ FastAPI ä¸­æš´éœ²ç®¡ç†ç«¯ç‚¹
@router.post("/admin/config/log-level")
async def set_log_level(level: str):
    config_mgr.set_log_level(level)
    return {"ok": True}
```

---

### 5. **ç¼ºä¹å¤šç§Ÿæˆ·æ”¯æŒ** ğŸŸ¡

**é—®é¢˜**ï¼š
- å½“å‰æ¶æ„å•ç§Ÿæˆ·
- æ— æ³•åœ¨åŒä¸€å®ä¾‹ä¸­éš”ç¦»å¤šä¸ªç”¨æˆ·

**æ”¹è¿›**ï¼ˆé˜¶æ®µä¸‰ï¼‰ï¼š
```python
# æ·»åŠ  tenant_id åˆ° Job
class Job(BaseModel):
    tenant_id: str
    job_id: str
    ...

# åœ¨ JobStore ä¸­å¼ºåˆ¶ tenant éš”ç¦»
def _resolve_job_dir(self, *, tenant_id: str, job_id: str) -> Path:
    return self._jobs_dir / tenant_id / job_id[:2] / job_id
```

---

## è¿ç»´ä¸å¯è§‚æµ‹æ€§

### 1. **ç¼ºä¹ Metrics å¯¼å‡º** ğŸ”´

**é—®é¢˜**ï¼š
- æ— æ³•äº†è§£ç³»ç»Ÿå®æ—¶çŠ¶æ€ï¼ˆjob å¤„ç†é€Ÿç‡ã€é”™è¯¯ç‡ç­‰ï¼‰
- æ— æ³•é…ç½®å‘Šè­¦

**æ”¹è¿›**ï¼š
```python
# src/infra/metrics.py
from prometheus_client import Counter, Histogram, Gauge

job_created = Counter("ss_job_created_total", "Total jobs created")
job_processing_seconds = Histogram("ss_job_processing_seconds", "Job processing time")
active_claims = Gauge("ss_active_claims", "Active queue claims")

# åœ¨ FastAPI ä¸­æš´éœ² Prometheus ç«¯ç‚¹
from prometheus_client import make_asgi_app
metrics_app = make_asgi_app()

app.mount("/metrics", metrics_app)
```

---

### 2. **ç¼ºä¹å¥åº·æ£€æŸ¥ç«¯ç‚¹** ğŸŸ¡

**é—®é¢˜**ï¼š
- å®¹å™¨ç¼–æ’ç³»ç»Ÿï¼ˆK8sï¼‰æ— æ³•åˆ¤æ–­æœåŠ¡æ˜¯å¦å¥åº·
- æ— æ³•å®ç°è‡ªåŠ¨æ¢å¤

**æ”¹è¿›**ï¼š
```python
@router.get("/health/live")  # Kubernetes liveness probe
async def liveness() -> dict:
    return {"status": "ok"}

@router.get("/health/ready")  # Kubernetes readiness probe
async def readiness() -> dict:
    try:
        # æ£€æŸ¥ä¾èµ–
        job_store.load("dummy")  # ä¼šæŠ›å¼‚å¸¸ï¼Œä½†è¿™æ˜¯ä¸ºäº†æµ‹è¯•è¿æ¥
    except JobNotFoundError:
        return {"status": "ready"}
    except Exception as e:
        return {"status": "unhealthy", "reason": str(e)}, 503
```

---

### 3. **ç¼ºä¹åˆ†å¸ƒå¼è¿½è¸ªæ”¯æŒ** ğŸŸ¡

**é—®é¢˜**ï¼š
- å¤šä¸ª worker å¤„ç†åŒä¸€ jobï¼Œéš¾ä»¥è¿½è¸ªç«¯åˆ°ç«¯æµç¨‹
- æ—  trace IDã€æ—  span

**æ”¹è¿›**ï¼š
```python
# src/infra/tracing.py
from opentelemetry import trace, metrics
from opentelemetry.exporter.jaeger import JaegerExporter

tracer = trace.get_tracer(__name__)

# åœ¨å…³é”®ä½ç½®æ·»åŠ  span
@tracer.start_as_current_span("job_creation")
def create_job(...):
    ...

@tracer.start_as_current_span("job_processing")
def process_claim(claim: QueueClaim):
    ...
```

---

### 4. **ç¼ºä¹å®¡è®¡æ—¥å¿—** ğŸŸ¡

**é—®é¢˜**ï¼š
- æ— æ³•è¿½è¸ªè°åšäº†ä»€ä¹ˆï¼ˆç”¨æˆ·ä¿®æ”¹ã€ç³»ç»Ÿæ“ä½œï¼‰
- æ— åˆè§„æ€§è®°å½•

**æ”¹è¿›**ï¼š
```python
# src/infra/audit.py
class AuditLogger:
    def log_action(
        self,
        action: str,
        resource_type: str,
        resource_id: str,
        user_id: str,
        changes: dict,
    ) -> None:
        event = {
            "timestamp": utc_now().isoformat(),
            "action": action,
            "resource_type": resource_type,
            "resource_id": resource_id,
            "user_id": user_id,
            "changes": changes,
        }
        logger.info("AUDIT_EVENT", extra=event)

# åœ¨ API ä¸­ä½¿ç”¨
@router.post("/jobs/{job_id}/confirm")
async def confirm_job(...):
    audit.log_action(
        action="JOB_CONFIRMED",
        resource_type="job",
        resource_id=job_id,
        user_id=current_user.id,
        changes={"status": job.status.value}
    )
    ...
```

---

## API ä¸å‘åå…¼å®¹æ€§

### 1. **Response æ ¼å¼æ— ç‰ˆæœ¬éš”ç¦»** ğŸŸ¡

**é—®é¢˜**ï¼š
- å¦‚æœæ·»åŠ æ–°å­—æ®µåˆ°å“åº”ï¼Œä¼šç ´åä¾èµ–ç‰¹å®šå­—æ®µé¡ºåºçš„å®¢æˆ·ç«¯

**æ”¹è¿›**ï¼š
```python
# ä½¿ç”¨ envelope åŒ…è£…
class APIResponse(BaseModel, Generic[T]):
    data: T
    meta: dict = {}
    errors: list[dict] | None = None

@router.get("/jobs/{job_id}")
async def get_job(...) -> APIResponse[GetJobResponse]:
    job = ...
    return APIResponse(
        data=GetJobResponse.from_domain(job),
        meta={"api_version": "v1", "timestamp": utc_now().isoformat()}
    )
```

---

### 2. **ç¼ºä¹ Content-Type åå•†** ğŸŸ¡

**é—®é¢˜**ï¼š
- åªæ”¯æŒ JSONï¼Œæ— æ³•è¿”å› CSVã€Parquet ç­‰æ ¼å¼
- æ— æ³•åœ¨ä¸ç ´å API çš„æƒ…å†µä¸‹æ‰©å±•æ ¼å¼

**æ”¹è¿›**ï¼š
```python
@router.get("/jobs/{job_id}/artifacts/{artifact_id}/export")
async def export_artifact(
    job_id: str,
    artifact_id: str,
    format: str = Query("json", regex="^(json|csv|parquet)$"),
):
    data = ...
    if format == "csv":
        return StreamingResponse(
            content=convert_to_csv(data),
            media_type="text/csv"
        )
    elif format == "parquet":
        return StreamingResponse(...)
    else:
        return data
```

---

### 3. **ç¼ºä¹æˆç†Ÿçš„é”™è¯¯å“åº”æ ‡å‡†** ğŸŸ¡

**é—®é¢˜**ï¼š
- é”™è¯¯å“åº”ä¸ä¸€è‡´ï¼ˆæœ‰æ—¶æœ‰ error_codeï¼Œæœ‰æ—¶æ²¡æœ‰ï¼‰
- æ— æ ‡å‡†çš„é”™è¯¯æ–‡æ¡£

**æ”¹è¿›**ï¼š
```python
# RFC 7807: Problem Details for HTTP APIs
class ErrorDetail(BaseModel):
    type: str  # Error type URI
    title: str  # Human-readable title
    detail: str  # Detailed explanation
    status: int  # HTTP status code
    instance: str  # Request ID for tracing

# ç¤ºä¾‹
{
    "type": "https://api.example.com/errors/job-not-found",
    "title": "Job Not Found",
    "detail": "Job with id 'xyz' does not exist",
    "status": 404,
    "instance": "req-12345"
}
```

---

## æ€»ç»“ï¼šé—æ¼çš„æ”¹è¿›ï¼ˆæŒ‰æ€»å·¥ä½œé‡æ’åºï¼‰

| åºå· | é¡¹ç›® | ä¼˜å…ˆçº§ | å·¥ä½œé‡ | æ€»åˆ†æ•° |
|------|------|--------|--------|--------|
| 4 | æ•°æ®è¿ç§»/ç‰ˆæœ¬å‡çº§ | ğŸ”´ é«˜ | 6-8h | â­â­â­â­â­ |
| 5 | å¹¶å‘æ§åˆ¶ä¸ç«æ€ | ğŸ”´ é«˜ | 8-10h | â­â­â­â­â­ |
| 7 | åˆ†å¸ƒå¼éƒ¨ç½²ä¸€è‡´æ€§ | ğŸ”´ é«˜ | 16-24h | â­â­â­â­â­ |
| 1 | ç±»å‹æ³¨è§£å®Œæ•´æ€§ | ğŸŸ¡ ä¸­ | 3-4h | â­â­â­â˜†â˜† |
| 6 | ä¼˜é›…å…³é—­ | ğŸŸ¡ ä¸­ | 4-6h | â­â­â­â­â˜† |
| 8 | API ç‰ˆæœ¬ç®¡ç† | ğŸŸ¡ ä¸­ | 3-4h | â­â­â­â˜†â˜† |
| 2 | ä¾èµ–ç‰ˆæœ¬é”å®š | ğŸŸ¢ ä½ | 1-2h | â­â­â˜†â˜†â˜† |
| 3 | Python ç‰ˆæœ¬æ”¿ç­– | ğŸŸ¢ ä½ | 0.5h | â­â˜†â˜†â˜†â˜† |
| **æ€»è®¡** | | | **42-59h** | |

---

ç”Ÿæˆæ—¶é—´ï¼š2025-01-07  
è¡¥å……å®¡è®¡å‘˜ï¼šAmp AI Agent
